feature_extractor(
  (dec1): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (dec2): Sequential(
    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (dec3): Sequential(
    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (dec4): Sequential(
    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (dec5): Sequential(
    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (enc5): Sequential(
    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
  )
  (enc4): Sequential(
    (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
  )
  (enc3): Sequential(
    (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
  )
  (enc2): Sequential(
    (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
  )
  (enc1): Sequential(
    (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
  )
  (block0): ModuleList(
    (0): Block(
      (norm1): LayerNorm((8,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (q): Linear(in_features=8, out_features=8, bias=True)
        (kv): Linear(in_features=8, out_features=16, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=8, out_features=8, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (sr): Conv2d(8, 8, kernel_size=(16, 16), stride=(16, 16))
        (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((8,), eps=1e-06, elementwise_affine=True)
      (norm3): LayerNorm((8,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=8, out_features=16, bias=True)
        (dwconv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
        (act): GELU()
        (fc2): Linear(in_features=16, out_features=8, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (mlp3): Mlp(
        (fc1): Linear(in_features=8, out_features=16, bias=True)
        (dwconv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
        (act): GELU()
        (fc2): Linear(in_features=16, out_features=8, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm21): LayerNorm((8,), eps=1e-06, elementwise_affine=True)
      (attn2): Attention(
        (q): Linear(in_features=8, out_features=8, bias=True)
        (kv): Linear(in_features=8, out_features=16, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=8, out_features=8, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (sr): Conv2d(8, 8, kernel_size=(16, 16), stride=(16, 16))
        (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
      )
      (drop_path2): Identity()
      (norm22): LayerNorm((8,), eps=1e-06, elementwise_affine=True)
      (mlp2): Mlp(
        (fc1): Linear(in_features=8, out_features=16, bias=True)
        (dwconv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
        (act): GELU()
        (fc2): Linear(in_features=16, out_features=8, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (deconv): Sequential(
        (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
    )
    (1): Block(
      (norm1): LayerNorm((8,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (q): Linear(in_features=8, out_features=8, bias=True)
        (kv): Linear(in_features=8, out_features=16, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=8, out_features=8, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (sr): Conv2d(8, 8, kernel_size=(16, 16), stride=(16, 16))
        (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((8,), eps=1e-06, elementwise_affine=True)
      (norm3): LayerNorm((8,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=8, out_features=16, bias=True)
        (dwconv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
        (act): GELU()
        (fc2): Linear(in_features=16, out_features=8, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (mlp3): Mlp(
        (fc1): Linear(in_features=8, out_features=16, bias=True)
        (dwconv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
        (act): GELU()
        (fc2): Linear(in_features=16, out_features=8, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm21): LayerNorm((8,), eps=1e-06, elementwise_affine=True)
      (attn2): Attention(
        (q): Linear(in_features=8, out_features=8, bias=True)
        (kv): Linear(in_features=8, out_features=16, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=8, out_features=8, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (sr): Conv2d(8, 8, kernel_size=(16, 16), stride=(16, 16))
        (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
      )
      (drop_path2): DropPath()
      (norm22): LayerNorm((8,), eps=1e-06, elementwise_affine=True)
      (mlp2): Mlp(
        (fc1): Linear(in_features=8, out_features=16, bias=True)
        (dwconv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
        (act): GELU()
        (fc2): Linear(in_features=16, out_features=8, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (deconv): Sequential(
        (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
    )
  )
  (norm0): LayerNorm((8,), eps=1e-06, elementwise_affine=True)
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (q): Linear(in_features=128, out_features=128, bias=True)
        (kv): Linear(in_features=128, out_features=256, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (sr): Conv2d(128, 128, kernel_size=(8, 8), stride=(8, 8))
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (norm3): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (mlp3): Mlp(
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm21): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn2): Attention(
        (q): Linear(in_features=128, out_features=128, bias=True)
        (kv): Linear(in_features=128, out_features=256, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (sr): Conv2d(128, 128, kernel_size=(8, 8), stride=(8, 8))
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (drop_path2): DropPath()
      (norm22): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp2): Mlp(
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (deconv): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
    )
    (1): Block(
      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (q): Linear(in_features=128, out_features=128, bias=True)
        (kv): Linear(in_features=128, out_features=256, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (sr): Conv2d(128, 128, kernel_size=(8, 8), stride=(8, 8))
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (norm3): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (mlp3): Mlp(
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm21): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (attn2): Attention(
        (q): Linear(in_features=128, out_features=128, bias=True)
        (kv): Linear(in_features=128, out_features=256, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (sr): Conv2d(128, 128, kernel_size=(8, 8), stride=(8, 8))
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (drop_path2): DropPath()
      (norm22): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (mlp2): Mlp(
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (deconv): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
    )
  )
  (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (kv): Linear(in_features=256, out_features=512, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (sr): Conv2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=256, out_features=512, bias=True)
        (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
        (act): GELU()
        (fc2): Linear(in_features=512, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (mlp3): Mlp(
        (fc1): Linear(in_features=256, out_features=512, bias=True)
        (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
        (act): GELU()
        (fc2): Linear(in_features=512, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm21): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn2): Attention(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (kv): Linear(in_features=256, out_features=512, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (sr): Conv2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (drop_path2): DropPath()
      (norm22): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp2): Mlp(
        (fc1): Linear(in_features=256, out_features=512, bias=True)
        (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
        (act): GELU()
        (fc2): Linear(in_features=512, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (deconv): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
    )
    (1): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (kv): Linear(in_features=256, out_features=512, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (sr): Conv2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=256, out_features=512, bias=True)
        (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
        (act): GELU()
        (fc2): Linear(in_features=512, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (mlp3): Mlp(
        (fc1): Linear(in_features=256, out_features=512, bias=True)
        (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
        (act): GELU()
        (fc2): Linear(in_features=512, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm21): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn2): Attention(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (kv): Linear(in_features=256, out_features=512, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (sr): Conv2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (drop_path2): DropPath()
      (norm22): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp2): Mlp(
        (fc1): Linear(in_features=256, out_features=512, bias=True)
        (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
        (act): GELU()
        (fc2): Linear(in_features=512, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (deconv): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
    )
  )
  (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (q): Linear(in_features=512, out_features=512, bias=True)
        (kv): Linear(in_features=512, out_features=1024, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (sr): Conv2d(512, 512, kernel_size=(2, 2), stride=(2, 2))
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (norm3): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (mlp3): Mlp(
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm21): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn2): Attention(
        (q): Linear(in_features=512, out_features=512, bias=True)
        (kv): Linear(in_features=512, out_features=1024, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (sr): Conv2d(512, 512, kernel_size=(2, 2), stride=(2, 2))
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (drop_path2): DropPath()
      (norm22): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp2): Mlp(
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (deconv): Sequential(
        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
    )
    (1): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (q): Linear(in_features=512, out_features=512, bias=True)
        (kv): Linear(in_features=512, out_features=1024, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (sr): Conv2d(512, 512, kernel_size=(2, 2), stride=(2, 2))
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (norm3): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (mlp3): Mlp(
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm21): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn2): Attention(
        (q): Linear(in_features=512, out_features=512, bias=True)
        (kv): Linear(in_features=512, out_features=1024, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (sr): Conv2d(512, 512, kernel_size=(2, 2), stride=(2, 2))
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (drop_path2): DropPath()
      (norm22): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp2): Mlp(
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (deconv): Sequential(
        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
    )
  )
  (norm3): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (q): Linear(in_features=512, out_features=512, bias=True)
        (kv): Linear(in_features=512, out_features=1024, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (norm3): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (mlp3): Mlp(
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm21): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn2): Attention(
        (q): Linear(in_features=512, out_features=512, bias=True)
        (kv): Linear(in_features=512, out_features=1024, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path2): DropPath()
      (norm22): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp2): Mlp(
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (deconv): Sequential(
        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
    )
    (1): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (q): Linear(in_features=512, out_features=512, bias=True)
        (kv): Linear(in_features=512, out_features=1024, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (norm3): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (mlp3): Mlp(
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm21): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn2): Attention(
        (q): Linear(in_features=512, out_features=512, bias=True)
        (kv): Linear(in_features=512, out_features=1024, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path2): DropPath()
      (norm22): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp2): Mlp(
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (deconv): Sequential(
        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
    )
  )
  (norm4): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  (conv_c4): SegNetEnc(
    (encode): Sequential(
      (0): Upsample(scale_factor=2.0, mode=bilinear)
      (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): ReLU(inplace=True)
      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): ReLU(inplace=True)
      (5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (6): ReLU(inplace=True)
    )
  )
  (conv_c3): SegNetEnc(
    (encode): Sequential(
      (0): Upsample(scale_factor=2.0, mode=bilinear)
      (1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): ReLU(inplace=True)
      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): ReLU(inplace=True)
      (5): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (6): ReLU(inplace=True)
    )
  )
  (conv_c2): SegNetEnc(
    (encode): Sequential(
      (0): Upsample(scale_factor=2.0, mode=bilinear)
      (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): ReLU(inplace=True)
      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): ReLU(inplace=True)
      (5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (6): ReLU(inplace=True)
    )
  )
  (conv_c1): SegNetEnc(
    (encode): Sequential(
      (0): Upsample(scale_factor=2.0, mode=bilinear)
      (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): ReLU(inplace=True)
      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): ReLU(inplace=True)
      (5): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (6): ReLU(inplace=True)
    )
  )
  (conv_c0): SegNetEnc(
    (encode): Sequential(
      (0): Upsample(scale_factor=1.0, mode=bilinear)
      (1): Conv2d(72, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): ReLU(inplace=True)
      (3): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): ReLU(inplace=True)
      (5): Conv2d(36, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (6): ReLU(inplace=True)
    )
  )
  (linear_fuse): Sequential(
    (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (linear_pred): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))
  (ccp_G): CCP_Generator(
    (sgp): SegNetEnc(
      (encode): Sequential(
        (0): Upsample(scale_factor=1.0, mode=bilinear)
        (1): Conv2d(896, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): ReLU(inplace=True)
        (4): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ReLU(inplace=True)
        (7): Conv2d(448, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (9): ReLU(inplace=True)
      )
    )
    (ccp_G): Geneator(
      (layer1): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ReLU(inplace=True)
      )
      (layer2): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ReLU(inplace=True)
      )
      (layer3): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ReLU(inplace=True)
      )
    )
    (pr1): Sequential(
      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (pr2): Sequential(
      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (pr3): Sequential(
      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (cd1): Sequential(
    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (cd2): Sequential(
    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (cd3): Sequential(
    (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (cd4): Sequential(
    (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
  )
  (mp): Conv2d(512, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (pf): Conv2d(12, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (alpha0): Conv2d(512, 8, kernel_size=(1, 1), stride=(1, 1))
  (alpha1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
  (alpha2): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
  (alpha3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
  (alpha4): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
  (f1_deconv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))
  (_c4): Conv2d(512, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (_c3): Conv2d(256, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (_c2): Conv2d(128, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (_c1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (_c0): Conv2d(8, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (ad1): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))
  )
  (ad2): Sequential(
    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(128, 2, kernel_size=(1, 1), stride=(1, 1))
  )
  (ad3): Sequential(
    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))
  )
  (ad4): Sequential(
    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))
  )
  (ad5): Sequential(
    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))
  )
)